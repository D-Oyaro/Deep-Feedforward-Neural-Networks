{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transfer learning to classify Digits 5 to 9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to try and reuse pretrained layers of a DNN to classify digits 5 to 9 using a small subset of MNIST dataset (500 images, 100 per class). We aim to show that transfer learning helps to train models in situations where only a small dataset is available.\n",
    "We reuse parts of the model trained to classify digits 0 to 4 of MNIST. Training is done using Tensorflow's low-level API.\n",
    "Steps involved include THE FOLLOWING LISTED BELOW:\n",
    "* Download and preprocess MNIST dataset\n",
    "* Reuse all 5 hidden layers to train new model. The layers to reuse are frozen and their output cached\n",
    "* Reuse only the first two hidden layers and see if performance improves. Here also, the reused layers are frozen and their output cached\n",
    "* Try building new model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and preprocess MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACRCAYAAADTnUPWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h/GEHFzwzFIMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa48ad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "#digits 5 to 9 are mapped as follows: 5->0,6->1,7->2,8->3,9->4\n",
    "#extract digits 5 to 9 of training dataset (100 for each class)\n",
    "id5 = np.random.permutation(np.nonzero(mnist.train.labels == 5)[0])[:100]\n",
    "id6 = np.random.permutation(np.nonzero(mnist.train.labels == 6)[0])[:100]\n",
    "id7 = np.random.permutation(np.nonzero(mnist.train.labels == 7)[0])[:100]\n",
    "id8 = np.random.permutation(np.nonzero(mnist.train.labels == 8)[0])[:100]\n",
    "id9 = np.random.permutation(np.nonzero(mnist.train.labels == 9)[0])[:100]\n",
    "train_index_5_9 = np.hstack((id5,id6,id7,id8,id9))\n",
    "mnist_train_images_5_9 = mnist.train.images[train_index_5_9] \n",
    "mnist_train_labels_5_9 = mnist.train.labels[train_index_5_9] - 5\n",
    "\n",
    "#extract digits 5 to 9 of validation dataset\n",
    "valid_index_5_9 = np.nonzero(mnist.validation.labels >= 5)[0]\n",
    "mnist_valid_images_5_9 = mnist.validation.images[valid_index_5_9]\n",
    "mnist_valid_labels_5_9 = mnist.validation.labels[valid_index_5_9] - 5\n",
    "\n",
    "#extract digits 5 to 9 of test dataset\n",
    "test_index_5_9 = np.nonzero(mnist.test.labels >= 5)[0]\n",
    "mnist_test_images_5_9 = mnist.test.images[test_index_5_9]\n",
    "mnist_test_labels_5_9 = mnist.test.labels[test_index_5_9] - 5\n",
    "\n",
    "#display a sample of training, validation and test images\n",
    "plt.subplot(131)\n",
    "plt.imshow(mnist_train_images_5_9[98].reshape(28,28))\n",
    "plt.title('Training sample')\n",
    "plt.subplot(132)\n",
    "plt.imshow(mnist_valid_images_5_9[120].reshape(28,28))\n",
    "plt.title('Validation sample')\n",
    "plt.subplot(133)\n",
    "plt.imshow(mnist_test_images_5_9[75].reshape(28,28))\n",
    "plt.title('Test sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuse all five hidden layers\n",
    "Build new DNN to classify digits 5 to 9 by reusing all five hidden layers of model trained to claasify digits 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1/kernel:0\n",
      "hidden1/bias:0\n",
      "hidden2/kernel:0\n",
      "hidden2/bias:0\n",
      "hidden3/kernel:0\n",
      "hidden3/bias:0\n",
      "hidden4/kernel:0\n",
      "hidden4/bias:0\n",
      "hidden5/kernel:0\n",
      "hidden5/bias:0\n"
     ]
    }
   ],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "\n",
    "reuse_vars_dict = {}    \n",
    "#define variables to save and restore\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"hidden[12345]\")\n",
    "for var in reuse_vars:\n",
    "    print(var.name)\n",
    "    reuse_vars_dict[var.op.name] = var\n",
    "\n",
    "#Saver object for restoring variables\n",
    "original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "#define variables to train\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='logits')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the new model\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_final.ckpt\n",
      "500 Training Accuracy: 1.0 Validation Accuracy: 0.735463\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.780508\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.799345\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.809582\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.816134\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.820639\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.822277\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.825962\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.826372\n",
      "5000 Training Accuracy: 1.0 Validation Accuracy: 0.827191\n",
      "5500 Training Accuracy: 1.0 Validation Accuracy: 0.825962\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.822277\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.821458\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.81982\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.818182\n",
      "8000 Training Accuracy: 1.0 Validation Accuracy: 0.817772\n",
      "8500 Training Accuracy: 1.0 Validation Accuracy: 0.817363\n",
      "9000 Training Accuracy: 1.0 Validation Accuracy: 0.817772\n",
      "9500 Training Accuracy: 1.0 Validation Accuracy: 0.818591\n",
      "10000 Training Accuracy: 1.0 Validation Accuracy: 0.821048\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize all variables\n",
    "    original_saver.restore(sess,'./saved_models/model_0_4_final.ckpt') #restore variables to reuse\n",
    "    \n",
    "    #cache output from frozen layers\n",
    "    shuffle_idx = np.random.permutation(mnist_train_labels_5_9.shape[0])\n",
    "    hidden5_output = sess.run(hidden5,feed_dict={X:mnist_train_images_5_9})[shuffle_idx]\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        hidden5_output_batches = np.array_split( hidden5_output,batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffle_idx],batch_size)\n",
    "        for hidden5_output_batch,y_batch in zip(hidden5_output_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={hidden5:hidden5_output_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={hidden5:hidden5_output_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        new_saver.save(sess,'./saved_models/model_5_9.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    new_saver.save(sess,'./saved_models/model_5_9_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_final.ckpt\n",
      "Test Accuracy: 0.810327\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate accuracy of model on test dataset\n",
    "with tf.Session() as sess:\n",
    "    #restore saved model\n",
    "    new_saver.restore(sess,'./saved_models/model_5_9_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_final.ckpt\n",
      "Prediction: [8 7 7 7 7]\n",
      "Target: [8 5 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "# Let's do some predictions on the test dataset using our model\n",
    "with tf.Session() as sess:\n",
    "    new_saver.restore(sess,'./saved_models/model_5_9_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[330:335]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[330:335] + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get better accuracy reusing only the first two hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reusing first two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1/kernel:0\n",
      "hidden1/bias:0\n",
      "hidden2/kernel:0\n",
      "hidden2/bias:0\n"
     ]
    }
   ],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "\n",
    "reuse_vars_dict = {}    \n",
    "#define variables to save and restore\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"hidden[12]\")\n",
    "for var in reuse_vars:\n",
    "    print(var.name)\n",
    "    reuse_vars_dict[var.op.name] = var\n",
    "\n",
    "#Saver object for restoring variables\n",
    "original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "#define variables to train\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[345]|logits')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the new model\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_final.ckpt\n",
      "500 Training Accuracy: 1.0 Validation Accuracy: 0.865684\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.867322\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.888206\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.872236\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.845209\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.886159\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.90172\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.884111\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.882473\n",
      "5000 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "5500 Training Accuracy: 1.0 Validation Accuracy: 0.872645\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.891073\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.914414\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.909091\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.896806\n",
      "8000 Training Accuracy: 1.0 Validation Accuracy: 0.898034\n",
      "8500 Training Accuracy: 1.0 Validation Accuracy: 0.90991\n",
      "9000 Training Accuracy: 1.0 Validation Accuracy: 0.899263\n",
      "9500 Training Accuracy: 1.0 Validation Accuracy: 0.902948\n",
      "10000 Training Accuracy: 1.0 Validation Accuracy: 0.873055\n",
      "10500 Training Accuracy: 1.0 Validation Accuracy: 0.899263\n",
      "11000 Training Accuracy: 1.0 Validation Accuracy: 0.895987\n",
      "11500 Training Accuracy: 1.0 Validation Accuracy: 0.911548\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize all variables\n",
    "    original_saver.restore(sess,'./saved_models/model_0_4_final.ckpt') #restore variables to reuse\n",
    "    \n",
    "    #cache output from frozen layers\n",
    "    shuffle_idx = np.random.permutation(mnist_train_labels_5_9.shape[0])\n",
    "    hidden2_output = sess.run(hidden2,feed_dict={X:mnist_train_images_5_9})[shuffle_idx]\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        hidden2_output_batches = np.array_split( hidden2_output,batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffle_idx],batch_size)\n",
    "        for hidden2_output_batch,y_batch in zip(hidden2_output_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={hidden2:hidden2_output_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={hidden2:hidden2_output_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        new_saver.save(sess,'./saved_models/model_5_9_with_reuse_2layers.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    new_saver.save(sess,'./saved_models/model_5_9_with_reuse_2layers_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_with_reuse_2layers_final.ckpt\n",
      "Test Accuracy: 0.908455\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate accuracy of model on test dataset\n",
    "with tf.Session() as sess:\n",
    "    #restore saved model\n",
    "    new_saver.restore(sess,'./saved_models/model_5_9_with_reuse_2layers_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_with_reuse_2layers_final.ckpt\n",
      "Prediction: [8 5 7 7 7]\n",
      "Target: [8 5 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "# Let's do some predictions on the test dataset using our model\n",
    "with tf.Session() as sess:\n",
    "    new_saver.restore(sess,'./saved_models/model_5_9_with_reuse_2layers_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[330:335]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[330:335] + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like by reusing only the first two hidden layers, we are able to train a more accurate model than when we reuse all the five hidden layers. Can we even do better training the model from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training DNN from scratch to classify digits 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the new model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.0 Validation Accuracy: 0.802211\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.853399\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.86855\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.898034\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.900082\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.816953\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.895577\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.864865\n",
      "5000 Training Accuracy: 1.0 Validation Accuracy: 0.921376\n",
      "5500 Training Accuracy: 1.0 Validation Accuracy: 0.879607\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.914824\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.911138\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.914414\n",
      "8000 Training Accuracy: 1.0 Validation Accuracy: 0.914414\n",
      "8500 Training Accuracy: 1.0 Validation Accuracy: 0.922195\n",
      "9000 Training Accuracy: 1.0 Validation Accuracy: 0.924242\n",
      "9500 Training Accuracy: 1.0 Validation Accuracy: 0.924652\n",
      "10000 Training Accuracy: 1.0 Validation Accuracy: 0.925061\n",
      "10500 Training Accuracy: 1.0 Validation Accuracy: 0.925471\n",
      "11000 Training Accuracy: 1.0 Validation Accuracy: 0.925061\n",
      "11500 Training Accuracy: 1.0 Validation Accuracy: 0.925471\n",
      "12000 Training Accuracy: 1.0 Validation Accuracy: 0.924242\n",
      "12500 Training Accuracy: 1.0 Validation Accuracy: 0.924652\n",
      "13000 Training Accuracy: 1.0 Validation Accuracy: 0.924652\n",
      "13500 Training Accuracy: 1.0 Validation Accuracy: 0.924652\n",
      "14000 Training Accuracy: 1.0 Validation Accuracy: 0.925061\n",
      "14500 Training Accuracy: 1.0 Validation Accuracy: 0.925471\n",
      "15000 Training Accuracy: 1.0 Validation Accuracy: 0.925471\n",
      "15500 Training Accuracy: 1.0 Validation Accuracy: 0.925061\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize all variables\n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        shuffle_idx = np.random.permutation(mnist_train_labels_5_9.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_5_9[shuffle_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffle_idx],batch_size)\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        saver.save(sess,'./saved_models/model_5_9_from_scratch.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    saver.save(sess,'./saved_models/model_5_9_from_scratch_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_from_scratch_final.ckpt\n",
      "Test Accuracy: 0.912775\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate accuracy of model on test dataset\n",
    "with tf.Session() as sess:\n",
    "    #restore saved model\n",
    "    saver.restore(sess,'./saved_models/model_5_9_from_scratch_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_5_9_from_scratch_final.ckpt\n",
      "Prediction: [8 5 7 7 7]\n",
      "Target: [8 5 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "# Let's do some predictions on the test dataset using our model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_5_9_from_scratch_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[330:335]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[330:335] + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems in this case training from scratch gives us the best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have seen that through transfer learning, we are able to train a new model with just a small dataset. While this gives us quite remarkable accuracy, training from scratch seems to perform better. Nevertheless transfer learning could prove extremely useful in cases where we are training a model to perform a complex task using a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tensaflo",
   "language": "python",
   "name": "tensaflo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
