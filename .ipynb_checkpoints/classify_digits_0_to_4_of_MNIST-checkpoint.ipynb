{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Digits 0 to 4 of MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In this project I train a deep neural network to classify the first five digits of the MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DNN to classify digits 0 to 4 of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build DNN with five hidden layers of 100 neurons each, with 5 output neurons to classify digits\n",
    "# 0 to 4 of MNIST\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#load and preprocess MNIST data\n",
    "import numpy as np\n",
    "\n",
    "valid_size = 5000\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "index_0_4 = np.random.permutation(np.nonzero(mnist.train.labels <= 4)[0])\n",
    "train_index_0_4 = index_0_4[:-valid_size]\n",
    "valid_index_0_4 = index_0_4[-valid_size:]\n",
    "\n",
    "mnist_train_images_0_4 = mnist.train.images[train_index_0_4]\n",
    "mnist_train_labels_0_4 = mnist.train.labels[train_index_0_4]\n",
    "\n",
    "mnist_valid_images_0_4 = mnist.train.images[valid_index_0_4]\n",
    "mnist_valid_labels_0_4 = mnist.train.labels[valid_index_0_4]\n",
    "\n",
    "\n",
    "test_index_0_4 = np.nonzero(mnist.test.labels <= 4)[0]\n",
    "mnist_test_images_0_4 = mnist.test.images[test_index_0_4]\n",
    "mnist_test_labels_0_4 = mnist.test.labels[test_index_0_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 1.0 Validation Accuracy: 0.9796\n",
      "1000 Training Accuracy: 0.978261 Validation Accuracy: 0.9818\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.9844\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.9794\n",
      "2500 Training Accuracy: 0.978261 Validation Accuracy: 0.9824\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.9842\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.9842\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "# Execution phase\n",
    "max_epoch_without_valid_improve = 5 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 500\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "x_batches = np.array_split(mnist_train_images_0_4,batch_size)\n",
    "y_batches = np.array_split(mnist_train_labels_0_4,batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        saver.save(sess,'./model1.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model1_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "Test Accuracy: 0.987741\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model1_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "Prediction: [1 2 3 4 1]\n",
      "Target: [1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model1_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_0_4[3000:3005]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[3000:3005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhP8H50Y4krKuB0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x92429e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show images corresponding to predicted digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(151)\n",
    "plt.imshow(mnist_test_images_0_4[3000].reshape(28,28))\n",
    "plt.subplot(152)\n",
    "plt.imshow(mnist_test_images_0_4[3001].reshape(28,28))\n",
    "plt.subplot(153)\n",
    "plt.imshow(mnist_test_images_0_4[3002].reshape(28,28))\n",
    "plt.subplot(154)\n",
    "plt.imshow(mnist_test_images_0_4[3003].reshape(28,28))\n",
    "plt.subplot(155)\n",
    "plt.imshow(mnist_test_images_0_4[3004].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's use Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build DNN with five hidden layers of 100 neurons each, with 5 output neurons to classify digits\n",
    "# 0 to 4 of MNIST using batch Normalization\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name=\"is_training\")\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,units=units)\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,momentum=0.9,training=is_training)\n",
    "    \n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1),name='bn1')\n",
    "    hidden2 = my_hidden_layer(bn1,name='hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2),name='bn2')\n",
    "    hidden3 = my_hidden_layer(bn2,name='hidden3')\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3),name='bn3')\n",
    "    hidden4 = my_hidden_layer(bn3,name='hidden4')\n",
    "    bn4 = tf.nn.elu(my_batch_norm_layer(hidden4),name='bn4')\n",
    "    hidden5 = my_hidden_layer(bn4,name='hidden5')\n",
    "    bn5 = tf.nn.elu(my_batch_norm_layer(hidden5),name='bn5')\n",
    "    logits_before_bn = tf.layers.dense(bn5,n_outputs,name='logits_before_bn')\n",
    "    logits = my_batch_norm_layer(logits_before_bn,name='logits')\n",
    "    \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.978261 Validation Accuracy: 0.9838\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.9848\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.9858\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.9876\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.9884\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.9878\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.985\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.9886\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.9888\n",
      "5000 Training Accuracy: 1.0 Validation Accuracy: 0.9858\n",
      "5500 Training Accuracy: 1.0 Validation Accuracy: 0.9878\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.9894\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.9894\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.9876\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.986\n",
      "8000 Training Accuracy: 1.0 Validation Accuracy: 0.9884\n",
      "8500 Training Accuracy: 0.978261 Validation Accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "# Execution phase\n",
    "max_epoch_without_valid_improve = 5 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 500\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "x_batches = np.array_split(mnist_train_images_0_4,batch_size)\n",
    "y_batches = np.array_split(mnist_train_labels_0_4,batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run([training_op,extra_update_ops],feed_dict={is_training:True,X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={is_training:False,X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={is_training:False,X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        saver.save(sess,'./model2.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model2_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model2_final.ckpt\n",
      "Test Accuracy: 0.989492\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model2_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={is_training:False,X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model2_final.ckpt\n",
      "Prediction: [1 2 3 4 1]\n",
      "Target: [1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model2_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={is_training:False,X:mnist_test_images_0_4[3000:3005]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[3000:3005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's add dropout and see if we get better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build DNN with five hidden layers of 100 neurons each, with 5 output neurons to classify digits\n",
    "# 0 to 4 of MNIST using batch Normalization and dropout\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "dropout_rate = 0.5\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name=\"is_training\")\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,units=units)\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,momentum=0.9,training=is_training)\n",
    "    \n",
    "    x_drop = tf.layers.dropout(X,rate=dropout_rate,training=is_training,name='x_drop')\n",
    "    \n",
    "    hidden1 = my_hidden_layer(x_drop,name='hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1),name='bn1')\n",
    "    bn1_drop = tf.layers.dropout(bn1,rate=dropout_rate,training=is_training,name='bn1_drop')\n",
    "    \n",
    "    hidden2 = my_hidden_layer(bn1_drop,name='hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2),name='bn2')\n",
    "    bn2_drop = tf.layers.dropout(bn2,rate=dropout_rate,training=is_training,name='bn2_drop')\n",
    "    \n",
    "    hidden3 = my_hidden_layer(bn2_drop,name='hidden3')\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3),name='bn3')\n",
    "    bn3_drop = tf.layers.dropout(bn3,rate=dropout_rate,training=is_training,name='bn3_drop')\n",
    "    \n",
    "    hidden4 = my_hidden_layer(bn3_drop,name='hidden4')\n",
    "    bn4 = tf.nn.elu(my_batch_norm_layer(hidden4),name='bn4')\n",
    "    bn4_drop = tf.layers.dropout(bn4,rate=dropout_rate,training=is_training,name='bn4_drop') \n",
    "    \n",
    "    hidden5 = my_hidden_layer(bn4_drop,name='hidden5')\n",
    "    bn5 = tf.nn.elu(my_batch_norm_layer(hidden5),name='bn5')\n",
    "    bn5_drop = tf.layers.dropout(bn5,rate=dropout_rate,training=is_training,name='bn5_drop') \n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn5_drop,n_outputs,name='logits_before_bn')\n",
    "    logits = my_batch_norm_layer(logits_before_bn,name='logits')\n",
    "    \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.934783 Validation Accuracy: 0.9458\n",
      "1000 Training Accuracy: 0.934783 Validation Accuracy: 0.9528\n",
      "1500 Training Accuracy: 0.956522 Validation Accuracy: 0.9568\n",
      "2000 Training Accuracy: 0.956522 Validation Accuracy: 0.961\n",
      "2500 Training Accuracy: 0.956522 Validation Accuracy: 0.9618\n",
      "3000 Training Accuracy: 0.978261 Validation Accuracy: 0.9628\n",
      "3500 Training Accuracy: 0.978261 Validation Accuracy: 0.9648\n",
      "4000 Training Accuracy: 0.978261 Validation Accuracy: 0.965\n",
      "4500 Training Accuracy: 0.956522 Validation Accuracy: 0.9666\n",
      "5000 Training Accuracy: 0.978261 Validation Accuracy: 0.968\n",
      "5500 Training Accuracy: 0.978261 Validation Accuracy: 0.9686\n",
      "6000 Training Accuracy: 0.978261 Validation Accuracy: 0.9688\n",
      "6500 Training Accuracy: 0.978261 Validation Accuracy: 0.9706\n",
      "7000 Training Accuracy: 0.978261 Validation Accuracy: 0.9702\n",
      "7500 Training Accuracy: 0.978261 Validation Accuracy: 0.972\n",
      "8000 Training Accuracy: 0.978261 Validation Accuracy: 0.9716\n",
      "8500 Training Accuracy: 0.978261 Validation Accuracy: 0.974\n",
      "9000 Training Accuracy: 0.978261 Validation Accuracy: 0.9748\n",
      "9500 Training Accuracy: 0.978261 Validation Accuracy: 0.9758\n",
      "10000 Training Accuracy: 0.978261 Validation Accuracy: 0.9766\n",
      "10500 Training Accuracy: 0.978261 Validation Accuracy: 0.977\n",
      "11000 Training Accuracy: 0.978261 Validation Accuracy: 0.9768\n",
      "11500 Training Accuracy: 0.978261 Validation Accuracy: 0.9768\n",
      "12000 Training Accuracy: 1.0 Validation Accuracy: 0.9778\n",
      "12500 Training Accuracy: 0.978261 Validation Accuracy: 0.9788\n",
      "13000 Training Accuracy: 1.0 Validation Accuracy: 0.979\n",
      "13500 Training Accuracy: 1.0 Validation Accuracy: 0.978\n",
      "14000 Training Accuracy: 1.0 Validation Accuracy: 0.9786\n",
      "14500 Training Accuracy: 1.0 Validation Accuracy: 0.9786\n",
      "15000 Training Accuracy: 0.978261 Validation Accuracy: 0.979\n",
      "15500 Training Accuracy: 1.0 Validation Accuracy: 0.9794\n",
      "16000 Training Accuracy: 1.0 Validation Accuracy: 0.9794\n",
      "16500 Training Accuracy: 1.0 Validation Accuracy: 0.9788\n",
      "17000 Training Accuracy: 1.0 Validation Accuracy: 0.9804\n",
      "17500 Training Accuracy: 1.0 Validation Accuracy: 0.9798\n",
      "18000 Training Accuracy: 1.0 Validation Accuracy: 0.9818\n",
      "18500 Training Accuracy: 1.0 Validation Accuracy: 0.9798\n",
      "19000 Training Accuracy: 1.0 Validation Accuracy: 0.9806\n",
      "19500 Training Accuracy: 1.0 Validation Accuracy: 0.9798\n",
      "20000 Training Accuracy: 1.0 Validation Accuracy: 0.9814\n",
      "20500 Training Accuracy: 1.0 Validation Accuracy: 0.9828\n",
      "21000 Training Accuracy: 1.0 Validation Accuracy: 0.982\n",
      "21500 Training Accuracy: 1.0 Validation Accuracy: 0.9812\n",
      "22000 Training Accuracy: 1.0 Validation Accuracy: 0.9812\n",
      "22500 Training Accuracy: 1.0 Validation Accuracy: 0.9828\n",
      "23000 Training Accuracy: 1.0 Validation Accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# Execution phase\n",
    "max_epoch_without_valid_improve = 5 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 500\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "x_batches = np.array_split(mnist_train_images_0_4,batch_size)\n",
    "y_batches = np.array_split(mnist_train_labels_0_4,batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run([training_op,extra_update_ops],feed_dict={is_training:True,X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={is_training:False,X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={is_training:False,X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        saver.save(sess,'./model3.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model3_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model3_final.ckpt\n",
      "Test Accuracy: 0.989881\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization and dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model3_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={is_training:False,X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model3_final.ckpt\n",
      "Prediction: [1 2 3 4 1]\n",
      "Target: [1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization and dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model3_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={is_training:False,X:mnist_test_images_0_4[3000:3005]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[3000:3005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "Training a new DNN to classify digits 5 to 9 using pretained hidden layers of model that classifies digits 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess data for training. Using only 100 images per digit (5 to 9) for training\n",
    "# shift labels such that  5->0,6->1,7->2,8->3,9->4\n",
    "import numpy as np\n",
    "\n",
    "index_5 = np.random.permutation(np.nonzero(mnist.train.labels == 5)[0])[:100] #  get 100 images for 5\n",
    "index_6 = np.random.permutation(np.nonzero(mnist.train.labels == 6)[0])[:100] #  get 100 images for 6\n",
    "index_7 = np.random.permutation(np.nonzero(mnist.train.labels == 7)[0])[:100] #  get 100 images for 7\n",
    "index_8 = np.random.permutation(np.nonzero(mnist.train.labels == 8)[0])[:100] #  get 100 images for 8\n",
    "index_9 = np.random.permutation(np.nonzero(mnist.train.labels == 9)[0])[:100] #  get 100 images for 9\n",
    "train_index_5_9 = np.random.permutation(np.hstack((index_5,index_6,index_7,index_8,index_9)))\n",
    "\n",
    "#training set\n",
    "mnist_train_images_5_9 = mnist.train.images[train_index_5_9]\n",
    "mnist_train_labels_5_9 = mnist.train.labels[train_index_5_9] - 5\n",
    "\n",
    "#validation set\n",
    "valid_index_5_9 = np.random.permutation(np.nonzero(mnist.validation.labels >= 5)[0]) \n",
    "mnist_valid_images_5_9 = mnist.validation.images[valid_index_5_9]\n",
    "mnist_valid_labels_5_9 = mnist.validation.labels[valid_index_5_9] - 5\n",
    "\n",
    "#test set\n",
    "test_index_5_9 = np.random.permutation(np.nonzero(mnist.test.labels >= 5)[0]) \n",
    "mnist_test_images_5_9 = mnist.test.images[test_index_5_9]\n",
    "mnist_test_labels_5_9 = mnist.test.labels[test_index_5_9] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xedd5e10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACFCAYAAABL2gNbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCzoRkSVY0ImILMGCTkRkif8D76g7Ajrm2OUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeadf6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some sample images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(131)\n",
    "plt.imshow(mnist_train_images_5_9[17].reshape(28,28))\n",
    "plt.subplot(132)\n",
    "plt.imshow(mnist_valid_images_5_9[1031].reshape(28,28))\n",
    "plt.subplot(133)\n",
    "plt.imshow(mnist_test_images_5_9[3999].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a new DNN to classify digits 5 to 9 using  all 5 pretained hidden layers of the previous batch normalization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new DNN to classify digits 5 to 9 using pretained hidden layers of the previous batch normalization model\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    #define variables to save and restore\n",
    "    reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"hidden[12345]\")\n",
    "    reuse_vars_dict = dict([(var.op.name,var) for var in reuse_vars])\n",
    "    #print(reuse_vars_dict)\n",
    "    original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "    \n",
    "    #define variables to train\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "50 Training Accuracy: 0.4 Validation Accuracy: 0.473792\n",
      "100 Training Accuracy: 0.5 Validation Accuracy: 0.603604\n",
      "150 Training Accuracy: 0.4 Validation Accuracy: 0.665438\n",
      "200 Training Accuracy: 0.7 Validation Accuracy: 0.701474\n",
      "250 Training Accuracy: 0.8 Validation Accuracy: 0.716626\n",
      "300 Training Accuracy: 0.8 Validation Accuracy: 0.744472\n",
      "350 Training Accuracy: 0.7 Validation Accuracy: 0.753071\n",
      "400 Training Accuracy: 0.8 Validation Accuracy: 0.768632\n",
      "450 Training Accuracy: 0.9 Validation Accuracy: 0.77846\n",
      "500 Training Accuracy: 0.8 Validation Accuracy: 0.785422\n",
      "550 Training Accuracy: 0.9 Validation Accuracy: 0.79484\n",
      "600 Training Accuracy: 1.0 Validation Accuracy: 0.798935\n",
      "650 Training Accuracy: 0.8 Validation Accuracy: 0.80344\n",
      "700 Training Accuracy: 0.8 Validation Accuracy: 0.802621\n",
      "750 Training Accuracy: 0.8 Validation Accuracy: 0.810401\n",
      "800 Training Accuracy: 1.0 Validation Accuracy: 0.818591\n",
      "850 Training Accuracy: 0.8 Validation Accuracy: 0.81941\n",
      "900 Training Accuracy: 0.9 Validation Accuracy: 0.820639\n",
      "950 Training Accuracy: 0.9 Validation Accuracy: 0.823915\n",
      "1000 Training Accuracy: 0.8 Validation Accuracy: 0.827191\n",
      "1050 Training Accuracy: 0.9 Validation Accuracy: 0.831286\n",
      "1100 Training Accuracy: 1.0 Validation Accuracy: 0.833743\n",
      "1150 Training Accuracy: 0.9 Validation Accuracy: 0.834971\n",
      "1200 Training Accuracy: 0.9 Validation Accuracy: 0.837838\n",
      "1250 Training Accuracy: 0.8 Validation Accuracy: 0.839885\n",
      "1300 Training Accuracy: 0.9 Validation Accuracy: 0.839476\n",
      "1350 Training Accuracy: 1.0 Validation Accuracy: 0.845618\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.845618\n",
      "1450 Training Accuracy: 0.9 Validation Accuracy: 0.841933\n",
      "1500 Training Accuracy: 0.9 Validation Accuracy: 0.846437\n",
      "1550 Training Accuracy: 1.0 Validation Accuracy: 0.847256\n",
      "1600 Training Accuracy: 1.0 Validation Accuracy: 0.847256\n",
      "1650 Training Accuracy: 1.0 Validation Accuracy: 0.849713\n",
      "1700 Training Accuracy: 0.9 Validation Accuracy: 0.850532\n",
      "1750 Training Accuracy: 1.0 Validation Accuracy: 0.850942\n",
      "1800 Training Accuracy: 0.9 Validation Accuracy: 0.847666\n",
      "1850 Training Accuracy: 0.9 Validation Accuracy: 0.851761\n",
      "1900 Training Accuracy: 0.9 Validation Accuracy: 0.85217\n",
      "1950 Training Accuracy: 1.0 Validation Accuracy: 0.854627\n",
      "2000 Training Accuracy: 0.9 Validation Accuracy: 0.853399\n",
      "2050 Training Accuracy: 1.0 Validation Accuracy: 0.854627\n",
      "2100 Training Accuracy: 0.9 Validation Accuracy: 0.85217\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.851761\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.855037\n",
      "2250 Training Accuracy: 0.8 Validation Accuracy: 0.856675\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.855037\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.859541\n",
      "2400 Training Accuracy: 1.0 Validation Accuracy: 0.858722\n",
      "2450 Training Accuracy: 1.0 Validation Accuracy: 0.857494\n",
      "2500 Training Accuracy: 0.9 Validation Accuracy: 0.858313\n",
      "2550 Training Accuracy: 1.0 Validation Accuracy: 0.859541\n",
      "2600 Training Accuracy: 1.0 Validation Accuracy: 0.857084\n",
      "2650 Training Accuracy: 1.0 Validation Accuracy: 0.856675\n",
      "2700 Training Accuracy: 1.0 Validation Accuracy: 0.859132\n",
      "2750 Training Accuracy: 1.0 Validation Accuracy: 0.857903\n",
      "2800 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "2850 Training Accuracy: 1.0 Validation Accuracy: 0.858313\n",
      "2900 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "2950 Training Accuracy: 0.9 Validation Accuracy: 0.859541\n",
      "3000 Training Accuracy: 0.9 Validation Accuracy: 0.861179\n",
      "3050 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "3100 Training Accuracy: 1.0 Validation Accuracy: 0.859951\n",
      "3150 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "3200 Training Accuracy: 0.9 Validation Accuracy: 0.861998\n",
      "3250 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "3300 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "3350 Training Accuracy: 1.0 Validation Accuracy: 0.862408\n",
      "3400 Training Accuracy: 1.0 Validation Accuracy: 0.862408\n",
      "3450 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.86036\n",
      "3550 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "3600 Training Accuracy: 1.0 Validation Accuracy: 0.857494\n",
      "3650 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "3700 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "3750 Training Accuracy: 1.0 Validation Accuracy: 0.862817\n",
      "3800 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "3850 Training Accuracy: 1.0 Validation Accuracy: 0.864455\n",
      "3900 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "3950 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4050 Training Accuracy: 1.0 Validation Accuracy: 0.864865\n",
      "4100 Training Accuracy: 1.0 Validation Accuracy: 0.863636\n",
      "4150 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "4200 Training Accuracy: 0.9 Validation Accuracy: 0.861998\n",
      "4250 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4300 Training Accuracy: 1.0 Validation Accuracy: 0.862408\n",
      "4350 Training Accuracy: 1.0 Validation Accuracy: 0.859951\n",
      "4400 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4450 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.861179\n",
      "4550 Training Accuracy: 1.0 Validation Accuracy: 0.86036\n",
      "4600 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "4650 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "4700 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4750 Training Accuracy: 1.0 Validation Accuracy: 0.861998\n",
      "4800 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "4850 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "4900 Training Accuracy: 0.9 Validation Accuracy: 0.862408\n",
      "4950 Training Accuracy: 0.9 Validation Accuracy: 0.861589\n",
      "5000 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "5050 Training Accuracy: 1.0 Validation Accuracy: 0.861179\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    original_saver.restore(sess,'./model1_final.ckpt')\n",
    "    hidden5_outputs = sess.run(hidden5,feed_dict={X:mnist_train_images_5_9})\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(hidden5_outputs.shape[0])\n",
    "        hidden5_batches = np.array_split(hidden5_outputs[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for hidden5_batch,y_batch in zip(hidden5_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={hidden5:hidden5_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={hidden5:hidden5_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model4.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model4_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model4_final.ckpt\n",
      "Test Accuracy: 0.860934\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization and dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model4_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model4_final.ckpt\n",
      "Prediction: [6 9 7 9 6]\n",
      "Target: [6 9 7 9 6]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization and dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model4_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[2920:2925]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[2920:2925] + 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf36e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show images corresponding to predicted digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(151)\n",
    "plt.imshow(mnist_test_images_5_9[2920].reshape(28,28))\n",
    "plt.subplot(152)\n",
    "plt.imshow(mnist_test_images_5_9[2921].reshape(28,28))\n",
    "plt.subplot(153)\n",
    "plt.imshow(mnist_test_images_5_9[2922].reshape(28,28))\n",
    "plt.subplot(154)\n",
    "plt.imshow(mnist_test_images_5_9[2923].reshape(28,28))\n",
    "plt.subplot(155)\n",
    "plt.imshow(mnist_test_images_5_9[2924].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a new DNN to classify digits 5 to 9 using first 4 pretained hidden layers of the previous batch normalization model (model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    #define variables to save and restore\n",
    "    reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"hidden[1234]\")\n",
    "    reuse_vars_dict = dict([(var.op.name,var) for var in reuse_vars])\n",
    "    #print(reuse_vars_dict)\n",
    "    original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "    \n",
    "    #define variables to train\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[5]|logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "50 Training Accuracy: 0.8 Validation Accuracy: 0.75389\n",
      "100 Training Accuracy: 0.8 Validation Accuracy: 0.803849\n",
      "150 Training Accuracy: 0.9 Validation Accuracy: 0.820229\n",
      "200 Training Accuracy: 1.0 Validation Accuracy: 0.834152\n",
      "250 Training Accuracy: 0.9 Validation Accuracy: 0.846437\n",
      "300 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "350 Training Accuracy: 1.0 Validation Accuracy: 0.864455\n",
      "400 Training Accuracy: 1.0 Validation Accuracy: 0.866912\n",
      "450 Training Accuracy: 1.0 Validation Accuracy: 0.871007\n",
      "500 Training Accuracy: 0.9 Validation Accuracy: 0.875512\n",
      "550 Training Accuracy: 1.0 Validation Accuracy: 0.864046\n",
      "600 Training Accuracy: 1.0 Validation Accuracy: 0.87674\n",
      "650 Training Accuracy: 1.0 Validation Accuracy: 0.872236\n",
      "700 Training Accuracy: 1.0 Validation Accuracy: 0.872645\n",
      "750 Training Accuracy: 1.0 Validation Accuracy: 0.868141\n",
      "800 Training Accuracy: 1.0 Validation Accuracy: 0.871417\n",
      "850 Training Accuracy: 1.0 Validation Accuracy: 0.875102\n",
      "900 Training Accuracy: 1.0 Validation Accuracy: 0.873055\n",
      "950 Training Accuracy: 1.0 Validation Accuracy: 0.87674\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.878788\n",
      "1050 Training Accuracy: 1.0 Validation Accuracy: 0.879197\n",
      "1100 Training Accuracy: 1.0 Validation Accuracy: 0.882064\n",
      "1150 Training Accuracy: 1.0 Validation Accuracy: 0.881654\n",
      "1200 Training Accuracy: 1.0 Validation Accuracy: 0.87715\n",
      "1250 Training Accuracy: 1.0 Validation Accuracy: 0.880426\n",
      "1300 Training Accuracy: 1.0 Validation Accuracy: 0.881245\n",
      "1350 Training Accuracy: 1.0 Validation Accuracy: 0.881245\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.880016\n",
      "1450 Training Accuracy: 1.0 Validation Accuracy: 0.877559\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.880016\n",
      "1550 Training Accuracy: 1.0 Validation Accuracy: 0.881654\n",
      "1600 Training Accuracy: 1.0 Validation Accuracy: 0.881245\n",
      "1650 Training Accuracy: 1.0 Validation Accuracy: 0.884521\n",
      "1700 Training Accuracy: 1.0 Validation Accuracy: 0.88493\n",
      "1750 Training Accuracy: 1.0 Validation Accuracy: 0.884521\n",
      "1800 Training Accuracy: 1.0 Validation Accuracy: 0.882883\n",
      "1850 Training Accuracy: 1.0 Validation Accuracy: 0.882064\n",
      "1900 Training Accuracy: 1.0 Validation Accuracy: 0.888206\n",
      "1950 Training Accuracy: 1.0 Validation Accuracy: 0.88534\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.884111\n",
      "2050 Training Accuracy: 1.0 Validation Accuracy: 0.88534\n",
      "2100 Training Accuracy: 1.0 Validation Accuracy: 0.885749\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.885749\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "2250 Training Accuracy: 1.0 Validation Accuracy: 0.88493\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.886568\n",
      "2400 Training Accuracy: 1.0 Validation Accuracy: 0.888616\n",
      "2450 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.886978\n",
      "2550 Training Accuracy: 1.0 Validation Accuracy: 0.886978\n",
      "2600 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "2650 Training Accuracy: 1.0 Validation Accuracy: 0.885749\n",
      "2700 Training Accuracy: 1.0 Validation Accuracy: 0.887797\n",
      "2750 Training Accuracy: 1.0 Validation Accuracy: 0.886568\n",
      "2800 Training Accuracy: 1.0 Validation Accuracy: 0.886568\n",
      "2850 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "2900 Training Accuracy: 1.0 Validation Accuracy: 0.885749\n",
      "2950 Training Accuracy: 1.0 Validation Accuracy: 0.88534\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.886159\n",
      "3050 Training Accuracy: 1.0 Validation Accuracy: 0.888206\n",
      "3100 Training Accuracy: 1.0 Validation Accuracy: 0.887797\n",
      "3150 Training Accuracy: 1.0 Validation Accuracy: 0.887797\n",
      "3200 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "3250 Training Accuracy: 1.0 Validation Accuracy: 0.888206\n",
      "3300 Training Accuracy: 1.0 Validation Accuracy: 0.887387\n",
      "3350 Training Accuracy: 1.0 Validation Accuracy: 0.886978\n",
      "3400 Training Accuracy: 1.0 Validation Accuracy: 0.886568\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    original_saver.restore(sess,'./model1_final.ckpt')\n",
    "    hidden4_outputs = sess.run(hidden4,feed_dict={X:mnist_train_images_5_9})\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(hidden4_outputs.shape[0])\n",
    "        hidden4_batches = np.array_split(hidden4_outputs[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for hidden4_batch,y_batch in zip(hidden4_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={hidden4:hidden4_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={hidden4:hidden4_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model5.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model5_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model5_final.ckpt\n",
      "Test Accuracy: 0.882946\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization and dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model5_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model5_final.ckpt\n",
      "Prediction: [6 9 7 9 6]\n",
      "Target: [6 9 7 9 6]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization and dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model5_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[2920:2925]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[2920:2925] + 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a new DNN to classify digits 5 to 9 using first 2 pretained hidden layers of the previous batch normalization model (model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    #define variables to save and restore\n",
    "    reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"hidden[12]\")\n",
    "    reuse_vars_dict = dict([(var.op.name,var) for var in reuse_vars])\n",
    "    #print(reuse_vars_dict)\n",
    "    original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "    \n",
    "    #define variables to train\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[345]|logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "50 Training Accuracy: 1.0 Validation Accuracy: 0.837428\n",
      "100 Training Accuracy: 1.0 Validation Accuracy: 0.890254\n",
      "150 Training Accuracy: 1.0 Validation Accuracy: 0.891073\n",
      "200 Training Accuracy: 1.0 Validation Accuracy: 0.904177\n",
      "250 Training Accuracy: 1.0 Validation Accuracy: 0.905405\n",
      "300 Training Accuracy: 1.0 Validation Accuracy: 0.900491\n",
      "350 Training Accuracy: 1.0 Validation Accuracy: 0.90991\n",
      "400 Training Accuracy: 1.0 Validation Accuracy: 0.904996\n",
      "450 Training Accuracy: 1.0 Validation Accuracy: 0.911548\n",
      "500 Training Accuracy: 1.0 Validation Accuracy: 0.913595\n",
      "550 Training Accuracy: 1.0 Validation Accuracy: 0.911957\n",
      "600 Training Accuracy: 1.0 Validation Accuracy: 0.90991\n",
      "650 Training Accuracy: 1.0 Validation Accuracy: 0.911957\n",
      "700 Training Accuracy: 1.0 Validation Accuracy: 0.914824\n",
      "750 Training Accuracy: 1.0 Validation Accuracy: 0.915233\n",
      "800 Training Accuracy: 1.0 Validation Accuracy: 0.914414\n",
      "850 Training Accuracy: 1.0 Validation Accuracy: 0.914824\n",
      "900 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "950 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "1050 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "1100 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "1150 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "1200 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "1250 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "1300 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "1350 Training Accuracy: 1.0 Validation Accuracy: 0.915233\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "1450 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "1550 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "1600 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "1650 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "1700 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "1750 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "1800 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "1850 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "1900 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "1950 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.9181\n",
      "2050 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2100 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.9181\n",
      "2250 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2400 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2450 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2550 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2600 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "2650 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2700 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "2750 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "2800 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "2850 Training Accuracy: 1.0 Validation Accuracy: 0.918509\n",
      "2900 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "2950 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3050 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3100 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "3150 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "3200 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3250 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "3300 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "3350 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3400 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "3450 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "3550 Training Accuracy: 1.0 Validation Accuracy: 0.915643\n",
      "3600 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "3650 Training Accuracy: 1.0 Validation Accuracy: 0.916462\n",
      "3700 Training Accuracy: 1.0 Validation Accuracy: 0.917281\n",
      "3750 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n",
      "3800 Training Accuracy: 1.0 Validation Accuracy: 0.916052\n",
      "3850 Training Accuracy: 1.0 Validation Accuracy: 0.916871\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    original_saver.restore(sess,'./model1_final.ckpt')\n",
    "    hidden2_outputs = sess.run(hidden2,feed_dict={X:mnist_train_images_5_9})\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(hidden2_outputs.shape[0])\n",
    "        hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for hidden2_batch,y_batch in zip(hidden2_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={hidden2:hidden2_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={hidden2:hidden2_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model6.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model6_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model6_final.ckpt\n",
      "Test Accuracy: 0.916684\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization and dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model6_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model6_final.ckpt\n",
      "Prediction: [6 9 7 9 6]\n",
      "Target: [6 9 7 9 6]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization and dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model6_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[2920:2925]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[2920:2925] + 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try introducing **dropoot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name='is_training')\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer12 = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    my_hidden_layer  = partial(tf.layers.dense,kernel_initializer=he_init,units=units)\n",
    "    my_dropout = partial(tf.layers.dropout,rate=0.5,training=is_training)\n",
    "    \n",
    "    hidden1 = my_hidden_layer12(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer12(hidden1,name='hidden2')\n",
    "    bn2 = my_dropout(hidden2,name='bn2')\n",
    "    hidden3 = my_hidden_layer(bn2,name='hidden3')\n",
    "    bn3 = my_dropout(hidden3,name='bn3')\n",
    "    hidden4 = my_hidden_layer(bn3,name='hidden4')\n",
    "    bn4 = my_dropout(hidden4,name='bn4')\n",
    "    hidden5 = my_hidden_layer(bn4,name='hidden5')\n",
    "    bn5 = my_dropout(hidden5,name='bn5')\n",
    "    logits = tf.layers.dense(bn5,n_outputs,name='logits')\n",
    "    \n",
    "    #define variables to save and restore\n",
    "    reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"hidden[12]\")\n",
    "    reuse_vars_dict = dict([(var.op.name,var) for var in reuse_vars])\n",
    "    #print(reuse_vars_dict)\n",
    "    original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "    \n",
    "    #define variables to train\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='hidden[345]|logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op',var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model1_final.ckpt\n",
      "50 Training Accuracy: 0.6 Validation Accuracy: 0.6724\n",
      "100 Training Accuracy: 1.0 Validation Accuracy: 0.751433\n",
      "150 Training Accuracy: 0.6 Validation Accuracy: 0.787469\n",
      "200 Training Accuracy: 0.6 Validation Accuracy: 0.798526\n",
      "250 Training Accuracy: 0.9 Validation Accuracy: 0.799754\n",
      "300 Training Accuracy: 0.8 Validation Accuracy: 0.816953\n",
      "350 Training Accuracy: 0.9 Validation Accuracy: 0.801802\n",
      "400 Training Accuracy: 0.9 Validation Accuracy: 0.823096\n",
      "450 Training Accuracy: 1.0 Validation Accuracy: 0.825962\n",
      "500 Training Accuracy: 0.9 Validation Accuracy: 0.846437\n",
      "550 Training Accuracy: 0.9 Validation Accuracy: 0.835381\n",
      "600 Training Accuracy: 1.0 Validation Accuracy: 0.821867\n",
      "650 Training Accuracy: 0.8 Validation Accuracy: 0.821867\n",
      "700 Training Accuracy: 0.8 Validation Accuracy: 0.823505\n",
      "750 Training Accuracy: 0.9 Validation Accuracy: 0.833333\n",
      "800 Training Accuracy: 0.9 Validation Accuracy: 0.843571\n",
      "850 Training Accuracy: 0.9 Validation Accuracy: 0.84439\n",
      "900 Training Accuracy: 1.0 Validation Accuracy: 0.843161\n",
      "950 Training Accuracy: 1.0 Validation Accuracy: 0.839066\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.839066\n",
      "1050 Training Accuracy: 0.8 Validation Accuracy: 0.844799\n",
      "1100 Training Accuracy: 0.9 Validation Accuracy: 0.831286\n",
      "1150 Training Accuracy: 0.7 Validation Accuracy: 0.841114\n",
      "1200 Training Accuracy: 0.6 Validation Accuracy: 0.841114\n",
      "1250 Training Accuracy: 0.9 Validation Accuracy: 0.846847\n",
      "1300 Training Accuracy: 0.7 Validation Accuracy: 0.847666\n",
      "1350 Training Accuracy: 0.9 Validation Accuracy: 0.847256\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.839066\n",
      "1450 Training Accuracy: 0.7 Validation Accuracy: 0.845618\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.848485\n",
      "1550 Training Accuracy: 0.8 Validation Accuracy: 0.847256\n",
      "1600 Training Accuracy: 0.7 Validation Accuracy: 0.841933\n",
      "1650 Training Accuracy: 0.8 Validation Accuracy: 0.846437\n",
      "1700 Training Accuracy: 0.9 Validation Accuracy: 0.846028\n",
      "1750 Training Accuracy: 0.9 Validation Accuracy: 0.850123\n",
      "1800 Training Accuracy: 1.0 Validation Accuracy: 0.85217\n",
      "1850 Training Accuracy: 0.9 Validation Accuracy: 0.851761\n",
      "1900 Training Accuracy: 1.0 Validation Accuracy: 0.851761\n",
      "1950 Training Accuracy: 0.9 Validation Accuracy: 0.855037\n",
      "2000 Training Accuracy: 0.8 Validation Accuracy: 0.851761\n",
      "2050 Training Accuracy: 0.9 Validation Accuracy: 0.845618\n",
      "2100 Training Accuracy: 1.0 Validation Accuracy: 0.849304\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.854218\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.848894\n",
      "2250 Training Accuracy: 0.9 Validation Accuracy: 0.853808\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.853808\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.856265\n",
      "2400 Training Accuracy: 0.7 Validation Accuracy: 0.849304\n",
      "2450 Training Accuracy: 1.0 Validation Accuracy: 0.853399\n",
      "2500 Training Accuracy: 0.7 Validation Accuracy: 0.85258\n",
      "2550 Training Accuracy: 0.9 Validation Accuracy: 0.854218\n",
      "2600 Training Accuracy: 0.9 Validation Accuracy: 0.857494\n",
      "2650 Training Accuracy: 0.8 Validation Accuracy: 0.851351\n",
      "2700 Training Accuracy: 0.9 Validation Accuracy: 0.854627\n",
      "2750 Training Accuracy: 0.9 Validation Accuracy: 0.848894\n",
      "2800 Training Accuracy: 0.8 Validation Accuracy: 0.846847\n",
      "2850 Training Accuracy: 0.7 Validation Accuracy: 0.851761\n",
      "2900 Training Accuracy: 1.0 Validation Accuracy: 0.842752\n",
      "2950 Training Accuracy: 0.7 Validation Accuracy: 0.851351\n",
      "3000 Training Accuracy: 0.9 Validation Accuracy: 0.859541\n",
      "3050 Training Accuracy: 1.0 Validation Accuracy: 0.857494\n",
      "3100 Training Accuracy: 1.0 Validation Accuracy: 0.854218\n",
      "3150 Training Accuracy: 0.9 Validation Accuracy: 0.850942\n",
      "3200 Training Accuracy: 0.9 Validation Accuracy: 0.86036\n",
      "3250 Training Accuracy: 0.9 Validation Accuracy: 0.846028\n",
      "3300 Training Accuracy: 1.0 Validation Accuracy: 0.857903\n",
      "3350 Training Accuracy: 0.9 Validation Accuracy: 0.862817\n",
      "3400 Training Accuracy: 0.8 Validation Accuracy: 0.857903\n",
      "3450 Training Accuracy: 0.9 Validation Accuracy: 0.84439\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.855856\n",
      "3550 Training Accuracy: 1.0 Validation Accuracy: 0.862408\n",
      "3600 Training Accuracy: 1.0 Validation Accuracy: 0.863227\n",
      "3650 Training Accuracy: 1.0 Validation Accuracy: 0.844799\n",
      "3700 Training Accuracy: 0.8 Validation Accuracy: 0.859132\n",
      "3750 Training Accuracy: 0.9 Validation Accuracy: 0.859541\n",
      "3800 Training Accuracy: 0.9 Validation Accuracy: 0.864046\n",
      "3850 Training Accuracy: 0.9 Validation Accuracy: 0.854627\n",
      "3900 Training Accuracy: 1.0 Validation Accuracy: 0.858722\n",
      "3950 Training Accuracy: 0.9 Validation Accuracy: 0.861998\n",
      "4000 Training Accuracy: 0.9 Validation Accuracy: 0.863636\n",
      "4050 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4100 Training Accuracy: 0.9 Validation Accuracy: 0.862817\n",
      "4150 Training Accuracy: 0.9 Validation Accuracy: 0.86855\n",
      "4200 Training Accuracy: 1.0 Validation Accuracy: 0.873464\n",
      "4250 Training Accuracy: 1.0 Validation Accuracy: 0.871417\n",
      "4300 Training Accuracy: 1.0 Validation Accuracy: 0.857903\n",
      "4350 Training Accuracy: 1.0 Validation Accuracy: 0.862817\n",
      "4400 Training Accuracy: 0.9 Validation Accuracy: 0.86077\n",
      "4450 Training Accuracy: 1.0 Validation Accuracy: 0.862408\n",
      "4500 Training Accuracy: 0.9 Validation Accuracy: 0.864455\n",
      "4550 Training Accuracy: 0.9 Validation Accuracy: 0.866503\n",
      "4600 Training Accuracy: 0.9 Validation Accuracy: 0.868141\n",
      "4650 Training Accuracy: 1.0 Validation Accuracy: 0.861589\n",
      "4700 Training Accuracy: 1.0 Validation Accuracy: 0.871007\n",
      "4750 Training Accuracy: 0.9 Validation Accuracy: 0.86077\n",
      "4800 Training Accuracy: 1.0 Validation Accuracy: 0.856675\n",
      "4850 Training Accuracy: 0.8 Validation Accuracy: 0.852989\n",
      "4900 Training Accuracy: 0.8 Validation Accuracy: 0.86077\n",
      "4950 Training Accuracy: 0.9 Validation Accuracy: 0.863227\n",
      "5000 Training Accuracy: 0.9 Validation Accuracy: 0.864865\n",
      "5050 Training Accuracy: 0.7 Validation Accuracy: 0.861179\n",
      "5100 Training Accuracy: 0.8 Validation Accuracy: 0.863636\n",
      "5150 Training Accuracy: 0.9 Validation Accuracy: 0.873874\n",
      "5200 Training Accuracy: 0.8 Validation Accuracy: 0.862817\n",
      "5250 Training Accuracy: 0.9 Validation Accuracy: 0.853808\n",
      "5300 Training Accuracy: 0.9 Validation Accuracy: 0.858722\n",
      "5350 Training Accuracy: 1.0 Validation Accuracy: 0.856265\n",
      "5400 Training Accuracy: 0.8 Validation Accuracy: 0.856675\n",
      "5450 Training Accuracy: 1.0 Validation Accuracy: 0.866503\n",
      "5500 Training Accuracy: 0.8 Validation Accuracy: 0.864455\n",
      "5550 Training Accuracy: 0.9 Validation Accuracy: 0.859951\n",
      "5600 Training Accuracy: 0.8 Validation Accuracy: 0.863636\n",
      "5650 Training Accuracy: 0.9 Validation Accuracy: 0.868141\n",
      "5700 Training Accuracy: 0.8 Validation Accuracy: 0.867322\n",
      "5750 Training Accuracy: 0.9 Validation Accuracy: 0.870598\n",
      "5800 Training Accuracy: 0.9 Validation Accuracy: 0.869779\n",
      "5850 Training Accuracy: 1.0 Validation Accuracy: 0.86077\n",
      "5900 Training Accuracy: 0.7 Validation Accuracy: 0.869779\n",
      "5950 Training Accuracy: 0.9 Validation Accuracy: 0.862817\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.854627\n",
      "6050 Training Accuracy: 0.9 Validation Accuracy: 0.863636\n",
      "6100 Training Accuracy: 0.8 Validation Accuracy: 0.859132\n",
      "6150 Training Accuracy: 0.8 Validation Accuracy: 0.871417\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    original_saver.restore(sess,'./model1_final.ckpt')\n",
    "    hidden2_outputs = sess.run(hidden2,feed_dict={X:mnist_train_images_5_9})\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(hidden2_outputs.shape[0])\n",
    "        hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for hidden2_batch,y_batch in zip(hidden2_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={is_training:True,hidden2:hidden2_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={is_training:False,hidden2:hidden2_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={is_training:False,X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model7.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model7_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model7_final.ckpt\n",
      "Test Accuracy: 0.866077\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization and dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model7_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={is_training:False,X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model7_final.ckpt\n",
      "Prediction: [6 9 7 9 6]\n",
      "Target: [6 9 7 9 6]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization and dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model7_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={is_training:False,X:mnist_test_images_5_9[2920:2925]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[2920:2925] + 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try training DNN to classify 5 to 9 **from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new DNN to classify digits 5 to 9 from scratch\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Training Accuracy: 0.8 Validation Accuracy: 0.890254\n",
      "100 Training Accuracy: 0.9 Validation Accuracy: 0.879607\n",
      "150 Training Accuracy: 0.9 Validation Accuracy: 0.912776\n",
      "200 Training Accuracy: 1.0 Validation Accuracy: 0.913595\n",
      "250 Training Accuracy: 1.0 Validation Accuracy: 0.91769\n",
      "300 Training Accuracy: 1.0 Validation Accuracy: 0.922195\n",
      "350 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "400 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "450 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "500 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "550 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "600 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "650 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "700 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "750 Training Accuracy: 1.0 Validation Accuracy: 0.925471\n",
      "800 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "850 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "900 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "950 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1050 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1100 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1150 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1200 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "1250 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "1300 Training Accuracy: 1.0 Validation Accuracy: 0.927109\n",
      "1350 Training Accuracy: 1.0 Validation Accuracy: 0.927518\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "1450 Training Accuracy: 1.0 Validation Accuracy: 0.927109\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "1550 Training Accuracy: 1.0 Validation Accuracy: 0.927109\n",
      "1600 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "1650 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1700 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1750 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1800 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1850 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1900 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "1950 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.926699\n",
      "2050 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "2100 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "2250 Training Accuracy: 1.0 Validation Accuracy: 0.92629\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.92588\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(mnist_train_images_5_9.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_5_9[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model8.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    saver.save(sess,'./model8_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model8_final.ckpt\n",
      "Test Accuracy: 0.922444\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model8_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model8_final.ckpt\n",
      "Prediction: [6 9 7 9 6]\n",
      "Target: [6 9 7 9 6]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./model8_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[2920:2925]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[2920:2925] + 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## Classifying digits 5 to 9 with the whole MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess data for training. Using digits 5 to 9\n",
    "# shift labels such that  5->0,6->1,7->2,8->3,9->4\n",
    "import numpy as np\n",
    "\n",
    "#training set\n",
    "train_index_5_9 = np.random.permutation(np.nonzero(mnist.train.labels >= 5)[0]) \n",
    "mnist_train_images_5_9 = mnist.train.images[train_index_5_9]\n",
    "mnist_train_labels_5_9 = mnist.train.labels[train_index_5_9] - 5\n",
    "\n",
    "#validation set\n",
    "valid_index_5_9 = np.random.permutation(np.nonzero(mnist.validation.labels >= 5)[0]) \n",
    "mnist_valid_images_5_9 = mnist.validation.images[valid_index_5_9]\n",
    "mnist_valid_labels_5_9 = mnist.validation.labels[valid_index_5_9] - 5\n",
    "\n",
    "#test set\n",
    "test_index_5_9 = np.random.permutation(np.nonzero(mnist.test.labels >= 5)[0]) \n",
    "mnist_test_images_5_9 = mnist.test.images[test_index_5_9]\n",
    "mnist_test_labels_5_9 = mnist.test.labels[test_index_5_9] - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new DNN to classify digits 5 to 9 from scratch\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Training Accuracy: 0.974026 Validation Accuracy: 0.958231\n",
      "100 Training Accuracy: 0.975881 Validation Accuracy: 0.96683\n",
      "150 Training Accuracy: 0.979592 Validation Accuracy: 0.969697\n",
      "200 Training Accuracy: 0.972171 Validation Accuracy: 0.973792\n",
      "250 Training Accuracy: 0.987013 Validation Accuracy: 0.973792\n",
      "300 Training Accuracy: 0.988868 Validation Accuracy: 0.977068\n",
      "350 Training Accuracy: 0.994434 Validation Accuracy: 0.978296\n",
      "400 Training Accuracy: 0.992579 Validation Accuracy: 0.976249\n",
      "450 Training Accuracy: 0.994434 Validation Accuracy: 0.982801\n",
      "500 Training Accuracy: 0.998145 Validation Accuracy: 0.979934\n",
      "550 Training Accuracy: 0.996289 Validation Accuracy: 0.983211\n",
      "600 Training Accuracy: 0.998145 Validation Accuracy: 0.982801\n",
      "650 Training Accuracy: 0.998145 Validation Accuracy: 0.984029\n",
      "700 Training Accuracy: 0.998145 Validation Accuracy: 0.98362\n",
      "750 Training Accuracy: 1.0 Validation Accuracy: 0.982801\n",
      "800 Training Accuracy: 1.0 Validation Accuracy: 0.981163\n",
      "850 Training Accuracy: 0.998145 Validation Accuracy: 0.984439\n",
      "900 Training Accuracy: 1.0 Validation Accuracy: 0.984439\n",
      "950 Training Accuracy: 1.0 Validation Accuracy: 0.984848\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.984848\n",
      "1050 Training Accuracy: 1.0 Validation Accuracy: 0.984439\n",
      "1100 Training Accuracy: 1.0 Validation Accuracy: 0.985667\n",
      "1150 Training Accuracy: 1.0 Validation Accuracy: 0.983211\n",
      "1200 Training Accuracy: 1.0 Validation Accuracy: 0.984439\n",
      "1250 Training Accuracy: 1.0 Validation Accuracy: 0.984848\n",
      "1300 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1350 Training Accuracy: 1.0 Validation Accuracy: 0.985667\n",
      "1400 Training Accuracy: 1.0 Validation Accuracy: 0.986077\n",
      "1450 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1550 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1600 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1650 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "1700 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1750 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "1800 Training Accuracy: 1.0 Validation Accuracy: 0.986077\n",
      "1850 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "1900 Training Accuracy: 1.0 Validation Accuracy: 0.986077\n",
      "1950 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2050 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2100 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2150 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2200 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "2250 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2300 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2350 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n",
      "2400 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "2450 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "2550 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2600 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "2650 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2700 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2750 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2800 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "2850 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2900 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "2950 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "3050 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "3100 Training Accuracy: 1.0 Validation Accuracy: 0.986896\n",
      "3150 Training Accuracy: 1.0 Validation Accuracy: 0.987305\n",
      "3200 Training Accuracy: 1.0 Validation Accuracy: 0.986486\n"
     ]
    }
   ],
   "source": [
    "#execution phase\n",
    "max_epoch_without_valid_improve = 20 # stop training if validation accuracy does not improve in five successive epochs\n",
    "batch_size = 50\n",
    "global_step_count = 0\n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        shuffled_idx = np.random.permutation(mnist_train_images_5_9.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_5_9[shuffled_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_5_9[shuffled_idx],batch_size)\n",
    "        \n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        accu_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_5_9,y:mnist_valid_labels_5_9})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        new_saver.save(sess,'./model9.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    new_saver.save(sess,'./model9_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model9_final.ckpt\n",
      "Test Accuracy: 0.98416\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy \n",
    "with tf.Session() as sess:\n",
    "    new_saver.restore(sess,'./model9_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_5_9,y:mnist_test_labels_5_9})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model9_final.ckpt\n",
      "Prediction: [8 5 6 9 9]\n",
      "Target: [8 5 6 9 9]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data\n",
    "with tf.Session() as sess:\n",
    "    new_saver.restore(sess,'./model9_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_5_9[3873:3878]}),axis=1) + 5\n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_5_9[3873:3878] + 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPz1QJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf6195c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show images corresponding to predicted digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(151)\n",
    "plt.imshow(mnist_test_images_5_9[3873].reshape(28,28))\n",
    "plt.subplot(152)\n",
    "plt.imshow(mnist_test_images_5_9[3874].reshape(28,28))\n",
    "plt.subplot(153)\n",
    "plt.imshow(mnist_test_images_5_9[3875].reshape(28,28))\n",
    "plt.subplot(154)\n",
    "plt.imshow(mnist_test_images_5_9[3876].reshape(28,28))\n",
    "plt.subplot(155)\n",
    "plt.imshow(mnist_test_images_5_9[3877].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tensaflo",
   "language": "python",
   "name": "tensaflo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
