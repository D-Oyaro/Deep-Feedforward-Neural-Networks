{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Digits 0 to 4 of MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build and train a deep feed forward neural network to classify the \n",
    "first five digits, 0 to 4, of the MNIST dataset. \n",
    "\n",
    "Procedure:\n",
    "* Download and preprocess MNIST dataset\n",
    "* Build DNN and train on dataset\n",
    "* Evaluate test accuracy of model and make some predictions\n",
    "* Tweak model to see if we achieve better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and preprocess MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACRCAYAAADTnUPWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa466d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "valid_size = 5000\n",
    "\n",
    "#extract digits 0 to 4 of training dataset\n",
    "train_index_0_4 = np.nonzero(mnist.train.labels <= 4)[0]\n",
    "mnist_train_images_0_4 = mnist.train.images[train_index_0_4]\n",
    "mnist_train_labels_0_4 = mnist.train.labels[train_index_0_4]\n",
    "\n",
    "#extract digits 0 to 4 of validation dataset\n",
    "valid_index_0_4 = np.nonzero(mnist.validation.labels <= 4)[0]\n",
    "mnist_valid_images_0_4 = mnist.validation.images[valid_index_0_4]\n",
    "mnist_valid_labels_0_4 = mnist.validation.labels[valid_index_0_4]\n",
    "\n",
    "#extract digits 0 to 4 of test dataset\n",
    "test_index_0_4 = np.nonzero(mnist.test.labels <= 4)[0]\n",
    "mnist_test_images_0_4 = mnist.test.images[test_index_0_4]\n",
    "mnist_test_labels_0_4 = mnist.test.labels[test_index_0_4]\n",
    "\n",
    "#display a sample of training, validation and test images\n",
    "plt.subplot(131)\n",
    "plt.imshow(mnist_train_images_0_4[10].reshape(28,28))\n",
    "plt.title('Training sample')\n",
    "plt.subplot(132)\n",
    "plt.imshow(mnist_valid_images_0_4[999].reshape(28,28))\n",
    "plt.title('Validation sample')\n",
    "plt.subplot(133)\n",
    "plt.imshow(mnist_test_images_0_4[1060].reshape(28,28))\n",
    "plt.title('Test sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train DNN\n",
    "DNN to train consists of 5 hidden layers of 100 units each and an output layer of 5 units, one for each digit. Model is built and trained using low-level Tensorflow API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    hidden2 = my_hidden_layer(hidden1,name='hidden2')\n",
    "    hidden3 = my_hidden_layer(hidden2,name='hidden3')\n",
    "    hidden4 = my_hidden_layer(hidden3,name='hidden4')\n",
    "    hidden5 = my_hidden_layer(hidden4,name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='logits')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.964286 Validation Accuracy: 0.982408\n",
      "1000 Training Accuracy: 1.0 Validation Accuracy: 0.980453\n",
      "1500 Training Accuracy: 1.0 Validation Accuracy: 0.985536\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.975371\n",
      "2500 Training Accuracy: 1.0 Validation Accuracy: 0.985536\n",
      "3000 Training Accuracy: 1.0 Validation Accuracy: 0.98749\n",
      "3500 Training Accuracy: 1.0 Validation Accuracy: 0.992572\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.989054\n",
      "4500 Training Accuracy: 1.0 Validation Accuracy: 0.989054\n",
      "5000 Training Accuracy: 0.982143 Validation Accuracy: 0.9914\n",
      "5500 Training Accuracy: 0.982143 Validation Accuracy: 0.989445\n",
      "6000 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.989445\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.99179\n",
      "8000 Training Accuracy: 1.0 Validation Accuracy: 0.99179\n",
      "8500 Training Accuracy: 1.0 Validation Accuracy: 0.99179\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize variables\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        shuffle_idx = np.random.permutation(mnist_train_labels_0_4.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_0_4[shuffle_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_0_4[shuffle_idx],batch_size)\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        saver.save(sess,'./saved_models/model_0_4.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    saver.save(sess,'./saved_models/model_0_4_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_final.ckpt\n",
      "Test Accuracy: 0.993578\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate accuracy of model on test dataset\n",
    "with tf.Session() as sess:\n",
    "    #restore saved model\n",
    "    saver.restore(sess,'./saved_models/model_0_4_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_final.ckpt\n",
      "Prediction: [4 0 2 3 2 4]\n",
      "Target: [4 0 2 3 2 4]\n"
     ]
    }
   ],
   "source": [
    "# Let's do some predictions on the test dataset using our model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_0_4_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={X:mnist_test_images_0_4[1989:1995]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[1989:1995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our trained model is working great! Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Batch Normalization\n",
    "Let us tweak our model by adding batch normalization to see if training converges faster and we get a more accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name=\"is_training\")\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,momentum=0.9,training=is_training)\n",
    "    \n",
    "    \n",
    "    hidden1 = my_hidden_layer(X,name='hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1),name='bn1')\n",
    "    hidden2 = my_hidden_layer(bn1,name='hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2),name='bn2')\n",
    "    hidden3 = my_hidden_layer(bn2,name='hidden3')\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3),name='bn3')\n",
    "    hidden4 = my_hidden_layer(bn3,name='hidden4')\n",
    "    bn4 = tf.nn.elu(my_batch_norm_layer(hidden4),name='bn4')\n",
    "    hidden5 = my_hidden_layer(bn4,name='hidden5')\n",
    "    bn5 = tf.nn.elu(my_batch_norm_layer(hidden5),name='bn5')\n",
    "    logits_before_bn = tf.layers.dense(bn5,n_outputs,name='logits_before_bn')\n",
    "    logits = my_batch_norm_layer(logits_before_bn,name='logits')\n",
    "    \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.982143 Validation Accuracy: 0.977326\n",
      "1000 Training Accuracy: 0.982143 Validation Accuracy: 0.956998\n",
      "1500 Training Accuracy: 0.964286 Validation Accuracy: 0.961689\n",
      "2000 Training Accuracy: 1.0 Validation Accuracy: 0.969898\n",
      "2500 Training Accuracy: 0.982143 Validation Accuracy: 0.980063\n",
      "3000 Training Accuracy: 0.928571 Validation Accuracy: 0.971071\n",
      "3500 Training Accuracy: 0.946429 Validation Accuracy: 0.939015\n",
      "4000 Training Accuracy: 0.964286 Validation Accuracy: 0.952697\n",
      "4500 Training Accuracy: 0.946429 Validation Accuracy: 0.976544\n",
      "5000 Training Accuracy: 0.964286 Validation Accuracy: 0.971071\n",
      "5500 Training Accuracy: 1.0 Validation Accuracy: 0.981235\n",
      "6000 Training Accuracy: 0.964286 Validation Accuracy: 0.97498\n",
      "6500 Training Accuracy: 1.0 Validation Accuracy: 0.979281\n",
      "7000 Training Accuracy: 1.0 Validation Accuracy: 0.985145\n",
      "7500 Training Accuracy: 1.0 Validation Accuracy: 0.982017\n",
      "8000 Training Accuracy: 0.982143 Validation Accuracy: 0.972244\n",
      "8500 Training Accuracy: 1.0 Validation Accuracy: 0.985927\n",
      "9000 Training Accuracy: 1.0 Validation Accuracy: 0.973808\n",
      "9500 Training Accuracy: 1.0 Validation Accuracy: 0.977326\n",
      "10000 Training Accuracy: 1.0 Validation Accuracy: 0.986317\n",
      "10500 Training Accuracy: 1.0 Validation Accuracy: 0.984754\n",
      "11000 Training Accuracy: 1.0 Validation Accuracy: 0.983581\n",
      "11500 Training Accuracy: 0.982143 Validation Accuracy: 0.975762\n",
      "12000 Training Accuracy: 1.0 Validation Accuracy: 0.97889\n",
      "12500 Training Accuracy: 0.982143 Validation Accuracy: 0.977717\n",
      "13000 Training Accuracy: 1.0 Validation Accuracy: 0.982017\n",
      "13500 Training Accuracy: 0.982143 Validation Accuracy: 0.981235\n",
      "14000 Training Accuracy: 0.982143 Validation Accuracy: 0.976544\n",
      "14500 Training Accuracy: 1.0 Validation Accuracy: 0.983972\n",
      "15000 Training Accuracy: 0.982143 Validation Accuracy: 0.980063\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize variables\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        shuffle_idx = np.random.permutation(mnist_train_labels_0_4.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_0_4[shuffle_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_0_4[shuffle_idx],batch_size)\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={is_training:True,X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={is_training:False,X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={is_training:False,X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        saver.save(sess,'./saved_models/model_0_4_with_BN.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    saver.save(sess,'./saved_models/model_0_4_with_BN_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_with_BN_final.ckpt\n",
      "Test Accuracy: 0.981903\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of batch normalization model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_0_4_with_BN_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={is_training:False,X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_with_BN_final.ckpt\n",
      "Prediction: [4 0 2 3 2 4]\n",
      "Target: [4 0 2 3 2 4]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using batch normalization model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_0_4_with_BN_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={is_training:False,X:mnist_test_images_0_4[1989:1995]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[1989:1995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we do not gain adding batch normalization. Our original model still performs better and \n",
    "took fewer steps in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dropout\n",
    "\n",
    "Will using dropout help our model generalize better than originally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build phase of model\n",
    "\n",
    "units = 100\n",
    "n_outputs = 5\n",
    "n_inputs = 28 * 28 # each image of MNIST is 28 by 28 pixels\n",
    "dropout_rate = 0.5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')\n",
    "is_training = tf.placeholder(tf.bool,shape=(),name=\"is_training\")\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    #use He initialization for weight initialization and Elu for activation function\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_hidden_layer = partial(tf.layers.dense,kernel_initializer=he_init,activation=tf.nn.elu,units=units)\n",
    "    \n",
    "    \n",
    "    x_drop = tf.layers.dropout(X,rate=dropout_rate,training=is_training,name='x_drop')\n",
    "    hidden1 = my_hidden_layer(x_drop,name='hidden1')\n",
    "    hidden1_drop = tf.layers.dropout(hidden1,rate=dropout_rate,training=is_training,name='hidden1_drop')\n",
    "    hidden2 = my_hidden_layer(hidden1_drop,name='hidden2')\n",
    "    hidden2_drop = tf.layers.dropout(hidden2,rate=dropout_rate,training=is_training,name='hidden2_drop')\n",
    "    hidden3 = my_hidden_layer(hidden2_drop,name='hidden3')\n",
    "    hidden3_drop = tf.layers.dropout(hidden3,rate=dropout_rate,training=is_training,name='hidden3_drop')\n",
    "    hidden4 = my_hidden_layer(hidden3_drop,name='hidden4')\n",
    "    hidden4_drop = tf.layers.dropout(hidden4,rate=dropout_rate,training=is_training,name='hidden4_drop') \n",
    "    hidden5 = my_hidden_layer(hidden4_drop,name='hidden5')\n",
    "    hidden5_drop = tf.layers.dropout(hidden5,rate=dropout_rate,training=is_training,name='hidden5_drop') \n",
    "    logits = tf.layers.dense(hidden5_drop,n_outputs,name='logits')\n",
    "\n",
    "    \n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    #output layer uses softmax activation function\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    #optimizer is Adam optimizer with a learning rate of 0.001 \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss,name='training_op')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    #operation to evaluate accuracy of model\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='eval')\n",
    "    \n",
    "\n",
    "#operation to initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#used to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#path to where tensorboard log is saved\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tensorboard\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#save graph and  stats used on the tensorboard\n",
    "valid_accuracy = tf.placeholder(tf.float32,shape=())\n",
    "valid_summary = tf.summary.scalar('valid_accuracy',valid_accuracy)\n",
    "summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Training Accuracy: 0.946429 Validation Accuracy: 0.945661\n",
      "1000 Training Accuracy: 0.982143 Validation Accuracy: 0.959734\n",
      "1500 Training Accuracy: 0.946429 Validation Accuracy: 0.963643\n",
      "2000 Training Accuracy: 0.946429 Validation Accuracy: 0.96638\n",
      "2500 Training Accuracy: 0.982143 Validation Accuracy: 0.967553\n",
      "3000 Training Accuracy: 0.982143 Validation Accuracy: 0.969507\n",
      "3500 Training Accuracy: 0.910714 Validation Accuracy: 0.972635\n",
      "4000 Training Accuracy: 1.0 Validation Accuracy: 0.971462\n",
      "4500 Training Accuracy: 0.982143 Validation Accuracy: 0.973417\n",
      "5000 Training Accuracy: 0.982143 Validation Accuracy: 0.976935\n",
      "5500 Training Accuracy: 0.982143 Validation Accuracy: 0.975371\n",
      "6000 Training Accuracy: 0.964286 Validation Accuracy: 0.978499\n",
      "6500 Training Accuracy: 0.964286 Validation Accuracy: 0.977717\n",
      "7000 Training Accuracy: 0.964286 Validation Accuracy: 0.981626\n",
      "7500 Training Accuracy: 0.946429 Validation Accuracy: 0.979281\n",
      "8000 Training Accuracy: 0.982143 Validation Accuracy: 0.981235\n",
      "8500 Training Accuracy: 0.964286 Validation Accuracy: 0.982799\n",
      "9000 Training Accuracy: 0.982143 Validation Accuracy: 0.981626\n",
      "9500 Training Accuracy: 0.964286 Validation Accuracy: 0.984363\n",
      "10000 Training Accuracy: 1.0 Validation Accuracy: 0.983581\n",
      "10500 Training Accuracy: 0.964286 Validation Accuracy: 0.985145\n",
      "11000 Training Accuracy: 0.982143 Validation Accuracy: 0.983972\n",
      "11500 Training Accuracy: 0.964286 Validation Accuracy: 0.986317\n",
      "12000 Training Accuracy: 0.964286 Validation Accuracy: 0.984754\n",
      "12500 Training Accuracy: 0.982143 Validation Accuracy: 0.986708\n",
      "13000 Training Accuracy: 0.982143 Validation Accuracy: 0.985145\n",
      "13500 Training Accuracy: 1.0 Validation Accuracy: 0.987099\n",
      "14000 Training Accuracy: 0.982143 Validation Accuracy: 0.985536\n",
      "14500 Training Accuracy: 0.946429 Validation Accuracy: 0.984754\n",
      "15000 Training Accuracy: 1.0 Validation Accuracy: 0.985145\n",
      "15500 Training Accuracy: 1.0 Validation Accuracy: 0.985536\n",
      "16000 Training Accuracy: 0.964286 Validation Accuracy: 0.986708\n",
      "16500 Training Accuracy: 0.982143 Validation Accuracy: 0.987881\n",
      "17000 Training Accuracy: 0.982143 Validation Accuracy: 0.986317\n",
      "17500 Training Accuracy: 1.0 Validation Accuracy: 0.985927\n",
      "18000 Training Accuracy: 0.982143 Validation Accuracy: 0.988272\n",
      "18500 Training Accuracy: 1.0 Validation Accuracy: 0.987881\n",
      "19000 Training Accuracy: 0.982143 Validation Accuracy: 0.988272\n",
      "19500 Training Accuracy: 1.0 Validation Accuracy: 0.98749\n",
      "20000 Training Accuracy: 0.982143 Validation Accuracy: 0.988272\n",
      "20500 Training Accuracy: 1.0 Validation Accuracy: 0.989445\n",
      "21000 Training Accuracy: 1.0 Validation Accuracy: 0.98749\n",
      "21500 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "22000 Training Accuracy: 1.0 Validation Accuracy: 0.989836\n",
      "22500 Training Accuracy: 1.0 Validation Accuracy: 0.988272\n",
      "23000 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "23500 Training Accuracy: 1.0 Validation Accuracy: 0.986317\n",
      "24000 Training Accuracy: 0.982143 Validation Accuracy: 0.988272\n",
      "24500 Training Accuracy: 0.964286 Validation Accuracy: 0.989445\n",
      "25000 Training Accuracy: 0.982143 Validation Accuracy: 0.990227\n",
      "25500 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "26000 Training Accuracy: 1.0 Validation Accuracy: 0.988272\n",
      "26500 Training Accuracy: 1.0 Validation Accuracy: 0.98749\n",
      "27000 Training Accuracy: 1.0 Validation Accuracy: 0.989445\n",
      "27500 Training Accuracy: 1.0 Validation Accuracy: 0.9914\n",
      "28000 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "28500 Training Accuracy: 0.982143 Validation Accuracy: 0.987881\n",
      "29000 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "29500 Training Accuracy: 1.0 Validation Accuracy: 0.988272\n",
      "30000 Training Accuracy: 1.0 Validation Accuracy: 0.989836\n",
      "30500 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n",
      "31000 Training Accuracy: 1.0 Validation Accuracy: 0.989445\n",
      "31500 Training Accuracy: 0.982143 Validation Accuracy: 0.987881\n",
      "32000 Training Accuracy: 1.0 Validation Accuracy: 0.98749\n",
      "32500 Training Accuracy: 1.0 Validation Accuracy: 0.988663\n"
     ]
    }
   ],
   "source": [
    "# Training phase of model\n",
    "\n",
    "# stop training if validation accuracy does not improve in 10 epochs\n",
    "# using early stopping to minimize overfitting\n",
    "max_epoch_without_valid_improve = 10 \n",
    "\n",
    "batch_size = 500\n",
    "global_step_count = 0 \n",
    "epoch_count = 0\n",
    "best_accu_valid = 0\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize variables\n",
    "    \n",
    "    while epoch_count < max_epoch_without_valid_improve:\n",
    "        #generate random mini-batches to use for training in current epoch\n",
    "        shuffle_idx = np.random.permutation(mnist_train_labels_0_4.shape[0])\n",
    "        x_batches = np.array_split(mnist_train_images_0_4[shuffle_idx],batch_size)\n",
    "        y_batches = np.array_split(mnist_train_labels_0_4[shuffle_idx],batch_size)\n",
    "        for x_batch,y_batch in zip(x_batches,y_batches):\n",
    "            sess.run(training_op,feed_dict={is_training:True,X:x_batch,y:y_batch})\n",
    "            global_step_count += 1\n",
    "        #evaluate training and validation accuracy\n",
    "        accu_train = accuracy.eval(feed_dict={is_training:False,X:x_batch,y:y_batch})\n",
    "        accu_valid = accuracy.eval(feed_dict={is_training:False,X:mnist_valid_images_0_4,y:mnist_valid_labels_0_4})  \n",
    "        if accu_valid > best_accu_valid:\n",
    "            best_accu_valid = accu_valid\n",
    "            epoch_count = 0\n",
    "        else:\n",
    "            epoch_count += 1\n",
    "        #save current validation and step count to tensorboard\n",
    "        summary_str = valid_summary.eval(feed_dict={valid_accuracy:accu_valid})\n",
    "        summary_writer.add_summary(summary_str, global_step_count)\n",
    "        #save model generated so far\n",
    "        saver.save(sess,'./saved_models/model_0_4_with_dropout.ckpt')\n",
    "        print(global_step_count,'Training Accuracy:',accu_train,'Validation Accuracy:',accu_valid)\n",
    "    \n",
    "    # save final model at end of trainiing\n",
    "    saver.save(sess,'./saved_models/model_0_4_with_dropout_final.ckpt')\n",
    "        \n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_with_dropout_final.ckpt\n",
      "Test Accuracy: 0.99066\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of dropout model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_0_4_with_dropout_final.ckpt')\n",
    "    accu_test = accuracy.eval(feed_dict={is_training:False,X:mnist_test_images_0_4,y:mnist_test_labels_0_4})  \n",
    "    print('Test Accuracy:',accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/model_0_4_with_dropout_final.ckpt\n",
      "Prediction: [4 0 2 3 2 4]\n",
      "Target: [4 0 2 3 2 4]\n"
     ]
    }
   ],
   "source": [
    "# do some predictions on the test data using dropout model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./saved_models/model_0_4_with_dropout_final.ckpt')\n",
    "    label_pred = np.argmax(sess.run(logits,feed_dict={is_training:False,X:mnist_test_images_0_4[1989:1995]}),axis=1) \n",
    "    \n",
    "print('Prediction:',label_pred)\n",
    "print('Target:',mnist_test_labels_0_4[1989:1995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "whoops! Looks like the dropout model does not outperform our original model either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have successfully trained a deep feed forward neural network to classify digits 0 to 4 of MNIST dataset with less than 1% error. It is still possible to achieve better accuracy with other neural network architectures like **CNN**, but that is beyond the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tensaflo",
   "language": "python",
   "name": "tensaflo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
